{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use `dlt` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who created this?\n",
    "\n",
    "[David Waltz](https://github.com/DavidWalz/dlipr) at RWTH Aachen University originally created `dlipr` package. I took his lecture, 'Deep Learning in Physics Research' and studied the deep learning. The software used in the lecture was supposed to run only on the cluster of the university but I personally customized it as `dlt` (Deep Learning Tools) so that everyone can use on their own environment. \n",
    "\n",
    "#### About this note\n",
    "In this article, I explained how to use `dlt`. Following, I took an example of the dataset, **[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist/blob/master/README.ja.md)** and show you how dlt works effectively. Basically you do not have to understand the deep learning in advance, and if you encounter the parts where you cannot understand please skip them. If so, I would like you to have a look at the images which you will get.\n",
    "\n",
    "- I use ★ mark where the package is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use it?\n",
    "\n",
    "### Preparation\n",
    "\n",
    "Before doing anything, I will show you the version of the libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `dlt` package, please run\n",
    "\n",
    "```\n",
    "pip install dlt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning - Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I refered to [the sample code of MNIST](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★ Reading the dataset\n",
    "\n",
    "Fashion MNIST dataset can be read as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Fashion-MNIST dataset\n"
     ]
    }
   ],
   "source": [
    "data = dlt.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dlt, the following dataset is available:`CIFAR-10`, `CIFAR-100`, `MNIST`, `FASHION-MNIST`. Each file has own `load_data` method like above example.\n",
    "\n",
    "You can access the raw data as follows. Fashion-MNIST dataset is provided through the same format as well-known MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.train_images\n",
    "y_train = data.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = data.test_images\n",
    "y_test = data.test_labels\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-short/top' 'Trouser' 'Pullover' 'Dress' 'Coat' 'Sandal' 'Shirt'\n",
      " 'Sneaker' 'Bag' 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ You can examine how the true labels are distributed in `y_train`, `y_test`. The typical datasets are basically prepared the same number of the labels so that there is not bias of them. When we set up our own datasets, we need to do like this as much as possible in order to obtain correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value: 6000\n",
      "Median Value: 6000.0\n",
      "Variance: 0\n",
      "Standard Deviation: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlt.utils.plot_distribution_data(Y=data.train_labels, # set the target dataset\n",
    "                                 dataset_name='y_train', # its name\n",
    "                                 classes=data.classes, # class label\n",
    "                                 fname='dist_train.png') # output filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dist_train.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value: 6000\n",
      "Median Value: 6000.0\n",
      "Variance: 0\n",
      "Standard Deviation: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlt.utils.plot_distribution_data(Y=data.test_labels, \n",
    "                                 dataset_name='y_test', \n",
    "                                 classes=data.classes, \n",
    "                                 fname='dist_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dist_test.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★Visualize the sample images\n",
    "\n",
    "Before the deep learning actually, you may want to look at some sample images. This is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_examples(data=data, \n",
    "                          num_examples=5, # How many images show on the column (#row corresponds #category)\n",
    "                         fname='fashion_mnist_examples.png' # filename\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fashion_mnist_examples.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 287s 5ms/step - loss: 0.6303 - acc: 0.7782 - val_loss: 0.3950 - val_acc: 0.8582\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 350s 6ms/step - loss: 0.4031 - acc: 0.8613 - val_loss: 0.3346 - val_acc: 0.8796\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 288s 5ms/step - loss: 0.3473 - acc: 0.8801 - val_loss: 0.3061 - val_acc: 0.8914\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 305s 5ms/step - loss: 0.3183 - acc: 0.8886 - val_loss: 0.2975 - val_acc: 0.8944\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 311s 5ms/step - loss: 0.2944 - acc: 0.8961 - val_loss: 0.2782 - val_acc: 0.8981\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 308s 5ms/step - loss: 0.2781 - acc: 0.9011 - val_loss: 0.2638 - val_acc: 0.9045\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 300s 5ms/step - loss: 0.2643 - acc: 0.9069 - val_loss: 0.2577 - val_acc: 0.9073\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 294s 5ms/step - loss: 0.2505 - acc: 0.9104 - val_loss: 0.2504 - val_acc: 0.9090\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 309s 5ms/step - loss: 0.2386 - acc: 0.9166 - val_loss: 0.2386 - val_acc: 0.9139\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 303s 5ms/step - loss: 0.2289 - acc: 0.9198 - val_loss: 0.2389 - val_acc: 0.9131\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 308s 5ms/step - loss: 0.2210 - acc: 0.9216 - val_loss: 0.2369 - val_acc: 0.9164\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 302s 5ms/step - loss: 0.2122 - acc: 0.9246 - val_loss: 0.2242 - val_acc: 0.9207\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape([-1, 28, 28, 1])\n",
    "X_test = X_test.reshape([-1, 28, 28, 1])\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.224214684319\n",
      "Test accuracy: 0.9207\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ The graph of the loss function and accurary which shows the learning process is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_loss_and_accuracy(fit,  # instance of model.fit\n",
    "                                   fname='loss_and_accuracy_graph.png' #filename and path to save image\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss_and_accuracy_graph.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for the test set\n",
    "Yp = model.predict(X_test)\n",
    "yp = np.argmax(Yp, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★The classification accuracy for Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classification task, we sometimes want to know how much accuracy each images are classified. In that case, we often confused how to realize it. `dlt` has the convenient method to give good image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 images\n",
    "for i in range(10):\n",
    "    dlt.utils.plot_prediction(\n",
    "        Yp=Yp[i], # the predicted label for Test dataset\n",
    "        X=data.test_images[i], # the image on each label\n",
    "        y=data.test_labels[i], # the correct label \n",
    "        classes=data.classes, # the label name\n",
    "        top_n=False, # How many images from the top. If False, it shows all category\n",
    "        fname='test-%i.png' % i) # filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test-0.png\">\n",
    "<img src=\"test-1.png\">\n",
    "<img src=\"test-2.png\">\n",
    "<img src=\"test-3.png\">\n",
    "<img src=\"test-4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange (blue) bar shows the accuracy on the correct (wrong) label.\n",
    "\n",
    "Taking the last image as asn example, the calculation classifies `Shirt` with the accuracy over 90%, but `T-shirt/top` with the one around 5%.\n",
    "\n",
    "If you want to know the result on the whole, Confusion Matrix will help you understand well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ★Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_confusion_matrix(test_labels=data.test_labels, # the correct label (before converting one-hot vector)\n",
    "                                  y_pred=yp, # Yp after np.argmax\n",
    "                                  classes=data.classes, # the label name\n",
    "                                  title='confusion matrix', # title of the graph\n",
    "                                  fname='confusion_matrix.png' # filename\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical (horizontal) axis represents the correct (predicted) label. Looking from the horizontal axis, if we see `Shirt` label, for example, it was classified to `Shirt` correctly with 67.80%, but also wrongly classified to `T-shirt/top` with 6.50 %."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
